{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1debeb43",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a5b66b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import scipy as sp\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "from scipy.sparse.linalg import eigsh\n",
    "import sys\n",
    "from torch.optim import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabf8c6b",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b09f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Return a boolean mask of length l with True at positions in idx.\"\"\"\n",
    "    mask = np.zeros(l, dtype=np.bool_)   # np.bool is deprecated\n",
    "    mask[idx] = True\n",
    "    return mask\n",
    "\n",
    "\n",
    "def scipy_coo_to_torch_sparse(coo: sp.coo_matrix, device=None, dtype=torch.float32):\n",
    "    \"\"\"Convert a SciPy COO matrix to a torch.sparse_coo_tensor (coalesced).\"\"\"\n",
    "    coo = coo.tocoo()\n",
    "    indices = np.vstack((coo.row, coo.col))\n",
    "    i = torch.from_numpy(indices).long()\n",
    "    v = torch.from_numpy(coo.data).to(dtype)\n",
    "    shape = coo.shape\n",
    "    t = torch.sparse_coo_tensor(i, v, torch.Size(shape))\n",
    "    t = t.coalesce()\n",
    "    return t.to(device) if device else t\n",
    "\n",
    "\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetric normalization:  D^{-1/2} A D^{-1/2}  (keeps SciPy COO).\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1)).flatten()\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5, where=rowsum > 0)\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.0\n",
    "    D_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return (adj.dot(D_inv_sqrt).transpose().dot(D_inv_sqrt)).tocoo()\n",
    "\n",
    "\n",
    "def preprocess_adj_to_torch(adj, add_self_loops=True, device=None, dtype=torch.float32):\n",
    "    \"\"\"A_hat = normalize(A + I); return torch sparse.\"\"\"\n",
    "    if add_self_loops:\n",
    "        adj = adj + sp.eye(adj.shape[0], dtype=adj.dtype, format='coo')\n",
    "    adj_norm = normalize_adj(adj)\n",
    "    return scipy_coo_to_torch_sparse(adj_norm, device=device, dtype=dtype)\n",
    "\n",
    "\n",
    "def preprocess_features_to_dense(features, device=None, dtype=torch.float32):\n",
    "    \"\"\"Row-normalize features (SciPy) and return dense torch.FloatTensor [N, F].\"\"\"\n",
    "    rowsum = np.array(features.sum(1)).flatten()\n",
    "    r_inv = np.power(rowsum, -1, where=rowsum != 0)\n",
    "    r_inv[np.isinf(r_inv)] = 0.0\n",
    "    R_inv = sp.diags(r_inv)\n",
    "    feats_norm = R_inv.dot(features)          # still SciPy\n",
    "    # For Cora/Citeseer/Pubmed sizes, dense is fine:\n",
    "    feats_dense = torch.from_numpy(feats_norm.toarray()).to(dtype)\n",
    "    return feats_dense.to(device) if device else feats_dense\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data_as_torch(dataset_str, device=None, dtype=torch.float32):\n",
    "    \"\"\"\n",
    "    Load citation data (Cora/Citeseer/Pubmed) and return:\n",
    "      adj_torch_sparse, features_dense, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "    where labels (y_*) are torch.FloatTensor one-hot (for your masked loss) and masks are torch.BoolTensor.\n",
    "    \"\"\"\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for n in names:\n",
    "        with open(f\"data/ind.{dataset_str}.{n}\", 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(f\"data/ind.{dataset_str}.test.index\")\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset_str == 'citeseer':\n",
    "        # Fix isolated nodes\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder) + 1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range - min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range - min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    # Build feature matrix\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "\n",
    "    # Build adjacency from graph dict\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))  # SciPy sparse\n",
    "\n",
    "    # Labels (one-hot)\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    # Splits (standard GCN)\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y) + 500)\n",
    "\n",
    "    train_mask_np = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask_np = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask_np = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train_np = np.zeros_like(labels)\n",
    "    y_val_np = np.zeros_like(labels)\n",
    "    y_test_np = np.zeros_like(labels)\n",
    "    y_train_np[train_mask_np, :] = labels[train_mask_np, :]\n",
    "    y_val_np[val_mask_np, :] = labels[val_mask_np, :]\n",
    "    y_test_np[test_mask_np, :] = labels[test_mask_np, :]\n",
    "\n",
    "    # ---- conversions to torch ----\n",
    "    adj_torch = preprocess_adj_to_torch(adj, add_self_loops=True, device=device, dtype=dtype)\n",
    "    features_torch = preprocess_features_to_dense(features, device=device, dtype=dtype)\n",
    "\n",
    "    y_train = torch.from_numpy(y_train_np).to(dtype).to(device) if device else torch.from_numpy(y_train_np).to(dtype)\n",
    "    y_val   = torch.from_numpy(y_val_np).to(dtype).to(device)   if device else torch.from_numpy(y_val_np).to(dtype)\n",
    "    y_test  = torch.from_numpy(y_test_np).to(dtype).to(device)  if device else torch.from_numpy(y_test_np).to(dtype)\n",
    "\n",
    "    train_mask = torch.from_numpy(train_mask_np).to(torch.bool).to(device) if device else torch.from_numpy(train_mask_np).to(torch.bool)\n",
    "    val_mask   = torch.from_numpy(val_mask_np).to(torch.bool).to(device)   if device else torch.from_numpy(val_mask_np).to(torch.bool)\n",
    "    test_mask  = torch.from_numpy(test_mask_np).to(torch.bool).to(device)  if device else torch.from_numpy(test_mask_np).to(torch.bool)\n",
    "\n",
    "    return adj_torch, features_torch, y_train, y_val, y_test, train_mask, val_mask, test_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28365f0b",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b2d461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax_cross_entropy(preds, labels, mask):\n",
    "    \n",
    "    labels = labels.argmax(dim=1)\n",
    "    loss = func.cross_entropy(preds, labels, reduction='none')\n",
    "\n",
    "    mask = mask.float()\n",
    "    mask /= mask.mean()\n",
    "    loss *= mask\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def masked_accuracy(preds, labels, mask):\n",
    " \n",
    "\n",
    "    pred_classes = torch.argmax(preds, dim=1)\n",
    "    true_classes = torch.argmax(labels, dim=1)\n",
    "\n",
    "    correct_prediction = (pred_classes == true_classes).float()\n",
    "\n",
    "    # Convert mask to float and normalize it\n",
    "    mask = mask.float()\n",
    "    mask = mask / mask.mean()\n",
    "    correct_prediction *= mask\n",
    "\n",
    "    return correct_prediction.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a88dbac",
   "metadata": {},
   "source": [
    "# Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c3b010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias = True):\n",
    "        super(GraphConvolution,self).__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features,out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform(self.weight)\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "    \n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj,support) if adj.is_sparse else torch.mm(adj,support)\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "        \n",
    "        return output\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, class_num, dropout, bias= True):\n",
    "        super().__init__()\n",
    "        self.gcn_1 = GraphConvolution(in_features, hidden_dim, bias)\n",
    "        self.gcn_2 = GraphConvolution(hidden_dim, class_num, bias)  \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = func.relu(self.gcn_1(x, adj))\n",
    "        x = self.dropout(x)\n",
    "        x = self.gcn_2(x, adj)   # logits\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dbf279",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88f081e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train_gcn(dataset='cora',\n",
    "              hidden_dim=16,\n",
    "              dropout=0.5,\n",
    "              lr=0.01,\n",
    "              weight_decay=5e-4,\n",
    "              epochs=200,\n",
    "              patience=10,\n",
    "              seed=42):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "    # Load data (adj sparse torch, features dense torch, labels one-hot, masks bool)\n",
    "    adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = \\\n",
    "        load_data_as_torch(dataset, device=device, dtype=torch.float32)\n",
    "    \n",
    "  \n",
    "\n",
    "    num_nodes, in_features = features.shape\n",
    "    num_classes = y_train.shape[1]\n",
    "\n",
    "    model = GCN(in_features, hidden_dim, num_classes, dropout=dropout).to(device)\n",
    "\n",
    "    \n",
    "    optimizer = Adam([\n",
    "    {'params': model.gcn_1.parameters(), 'weight_decay': 5e-4},  # regularized\n",
    "    {'params': model.gcn_2.parameters(), 'weight_decay': 0.0}    # no regularization\n",
    "    ], lr=lr)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    best_val = -float('inf')\n",
    "    best_state = None\n",
    "    wait = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(features, adj)  # [N, C]\n",
    "        loss_train = masked_softmax_cross_entropy(logits, y_train, train_mask)\n",
    "        acc_train = masked_accuracy(logits, y_train, train_mask)\n",
    "\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ---- validation ----\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(features, adj)\n",
    "            loss_val = masked_softmax_cross_entropy(logits, y_val, val_mask)\n",
    "            acc_val = masked_accuracy(logits, y_val, val_mask)\n",
    "\n",
    "       \n",
    "\n",
    "        # Early stopping on validation accuracy (or use -loss)\n",
    "        score = acc_val.item()\n",
    "        if score > best_val:\n",
    "            best_val = score\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}. Best val acc: {best_val:.4f}\")\n",
    "                break\n",
    "\n",
    "    \n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(features, adj)\n",
    "        test_loss = masked_softmax_cross_entropy(logits, y_test, test_mask).item()\n",
    "        test_acc = masked_accuracy(logits, y_test, test_mask).item()\n",
    "    print(f\"Test  | Loss {test_loss:.4f} Acc {test_acc:.4f}\")\n",
    "\n",
    "    return model, {'val_acc': best_val, 'test_acc': test_acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "936a0eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2375/2649714054.py:71: DeprecationWarning: Please import `csr_matrix` from the `scipy.sparse` namespace; the `scipy.sparse.csr` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  objects.append(pkl.load(f, encoding='latin1'))\n",
      "/tmp/ipykernel_2375/3476770309.py:12: FutureWarning: `nn.init.xavier_uniform` is now deprecated in favor of `nn.init.xavier_uniform_`.\n",
      "  nn.init.xavier_uniform(self.weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 78. Best val acc: 0.7000\n",
      "Test  | Loss 1.3989 Acc 0.6950\n"
     ]
    }
   ],
   "source": [
    "model, metrics = train_gcn(dataset='citeseer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1f16c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5285/2649714054.py:71: DeprecationWarning: Please import `csr_matrix` from the `scipy.sparse` namespace; the `scipy.sparse.csr` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  objects.append(pkl.load(f, encoding='latin1'))\n",
      "/tmp/ipykernel_5285/2003895035.py:12: FutureWarning: `nn.init.xavier_uniform` is now deprecated in favor of `nn.init.xavier_uniform_`.\n",
      "  nn.init.xavier_uniform(self.weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train Loss 1.9459 Acc 0.1786 | Val Loss 1.9449 Acc 0.1960\n",
      "Epoch 002 | Train Loss 1.9409 Acc 0.2643 | Val Loss 1.9423 Acc 0.2640\n",
      "Epoch 003 | Train Loss 1.9341 Acc 0.4071 | Val Loss 1.9374 Acc 0.4340\n",
      "Epoch 004 | Train Loss 1.9259 Acc 0.6071 | Val Loss 1.9326 Acc 0.4580\n",
      "Epoch 005 | Train Loss 1.9149 Acc 0.5357 | Val Loss 1.9294 Acc 0.3640\n",
      "Epoch 006 | Train Loss 1.9093 Acc 0.5500 | Val Loss 1.9265 Acc 0.3460\n",
      "Epoch 007 | Train Loss 1.9004 Acc 0.4714 | Val Loss 1.9220 Acc 0.3480\n",
      "Epoch 008 | Train Loss 1.8863 Acc 0.4786 | Val Loss 1.9164 Acc 0.3540\n",
      "Epoch 009 | Train Loss 1.8754 Acc 0.5500 | Val Loss 1.9100 Acc 0.4040\n",
      "Epoch 010 | Train Loss 1.8651 Acc 0.5643 | Val Loss 1.9026 Acc 0.4560\n",
      "Epoch 011 | Train Loss 1.8587 Acc 0.6071 | Val Loss 1.8939 Acc 0.5300\n",
      "Epoch 012 | Train Loss 1.8428 Acc 0.6214 | Val Loss 1.8834 Acc 0.5880\n",
      "Epoch 013 | Train Loss 1.8259 Acc 0.7429 | Val Loss 1.8734 Acc 0.6240\n",
      "Epoch 014 | Train Loss 1.8109 Acc 0.6786 | Val Loss 1.8638 Acc 0.6420\n",
      "Epoch 015 | Train Loss 1.7947 Acc 0.7000 | Val Loss 1.8546 Acc 0.6760\n",
      "Epoch 016 | Train Loss 1.7866 Acc 0.7500 | Val Loss 1.8463 Acc 0.6880\n",
      "Epoch 017 | Train Loss 1.7638 Acc 0.7929 | Val Loss 1.8392 Acc 0.6920\n",
      "Epoch 018 | Train Loss 1.7447 Acc 0.8286 | Val Loss 1.8324 Acc 0.6940\n",
      "Epoch 019 | Train Loss 1.7529 Acc 0.7571 | Val Loss 1.8257 Acc 0.7020\n",
      "Epoch 020 | Train Loss 1.7167 Acc 0.7786 | Val Loss 1.8183 Acc 0.7100\n",
      "Epoch 021 | Train Loss 1.7114 Acc 0.8071 | Val Loss 1.8101 Acc 0.7100\n",
      "Epoch 022 | Train Loss 1.6970 Acc 0.8357 | Val Loss 1.8013 Acc 0.7120\n",
      "Epoch 023 | Train Loss 1.6753 Acc 0.8000 | Val Loss 1.7919 Acc 0.7140\n",
      "Epoch 024 | Train Loss 1.6441 Acc 0.8357 | Val Loss 1.7821 Acc 0.7160\n",
      "Epoch 025 | Train Loss 1.6262 Acc 0.8143 | Val Loss 1.7719 Acc 0.7160\n",
      "Epoch 026 | Train Loss 1.6173 Acc 0.8214 | Val Loss 1.7615 Acc 0.7200\n",
      "Epoch 027 | Train Loss 1.5898 Acc 0.8429 | Val Loss 1.7501 Acc 0.7280\n",
      "Epoch 028 | Train Loss 1.5731 Acc 0.8714 | Val Loss 1.7378 Acc 0.7280\n",
      "Epoch 029 | Train Loss 1.5489 Acc 0.8643 | Val Loss 1.7251 Acc 0.7360\n",
      "Epoch 030 | Train Loss 1.5356 Acc 0.8357 | Val Loss 1.7122 Acc 0.7380\n",
      "Epoch 031 | Train Loss 1.5260 Acc 0.8357 | Val Loss 1.7006 Acc 0.7340\n",
      "Epoch 032 | Train Loss 1.4914 Acc 0.8857 | Val Loss 1.6892 Acc 0.7320\n",
      "Epoch 033 | Train Loss 1.4773 Acc 0.8357 | Val Loss 1.6779 Acc 0.7360\n",
      "Epoch 034 | Train Loss 1.4764 Acc 0.8643 | Val Loss 1.6664 Acc 0.7380\n",
      "Epoch 035 | Train Loss 1.4383 Acc 0.8786 | Val Loss 1.6550 Acc 0.7380\n",
      "Epoch 036 | Train Loss 1.4040 Acc 0.8643 | Val Loss 1.6425 Acc 0.7380\n",
      "Epoch 037 | Train Loss 1.3705 Acc 0.8643 | Val Loss 1.6288 Acc 0.7440\n",
      "Epoch 038 | Train Loss 1.3847 Acc 0.8786 | Val Loss 1.6140 Acc 0.7540\n",
      "Epoch 039 | Train Loss 1.3470 Acc 0.8643 | Val Loss 1.5980 Acc 0.7560\n",
      "Epoch 040 | Train Loss 1.3169 Acc 0.8857 | Val Loss 1.5822 Acc 0.7660\n",
      "Epoch 041 | Train Loss 1.3119 Acc 0.8929 | Val Loss 1.5679 Acc 0.7600\n",
      "Epoch 042 | Train Loss 1.3032 Acc 0.8929 | Val Loss 1.5537 Acc 0.7600\n",
      "Epoch 043 | Train Loss 1.2782 Acc 0.8786 | Val Loss 1.5400 Acc 0.7620\n",
      "Epoch 044 | Train Loss 1.2499 Acc 0.8786 | Val Loss 1.5273 Acc 0.7620\n",
      "Epoch 045 | Train Loss 1.2038 Acc 0.8857 | Val Loss 1.5141 Acc 0.7600\n",
      "Epoch 046 | Train Loss 1.2112 Acc 0.9143 | Val Loss 1.5010 Acc 0.7580\n",
      "Epoch 047 | Train Loss 1.1632 Acc 0.9071 | Val Loss 1.4892 Acc 0.7520\n",
      "Epoch 048 | Train Loss 1.1869 Acc 0.8857 | Val Loss 1.4770 Acc 0.7520\n",
      "Epoch 049 | Train Loss 1.1145 Acc 0.9357 | Val Loss 1.4648 Acc 0.7540\n",
      "Epoch 050 | Train Loss 1.1386 Acc 0.8714 | Val Loss 1.4517 Acc 0.7540\n",
      "Early stopping at epoch 50. Best val acc: 0.7660\n",
      "Test  | Loss 1.5765 Acc 0.7770\n"
     ]
    }
   ],
   "source": [
    "# Cora\n",
    "model, m = train_gcn(dataset='cora')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "593f852b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5285/2649714054.py:71: DeprecationWarning: Please import `csr_matrix` from the `scipy.sparse` namespace; the `scipy.sparse.csr` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  objects.append(pkl.load(f, encoding='latin1'))\n",
      "/tmp/ipykernel_5285/2003895035.py:12: FutureWarning: `nn.init.xavier_uniform` is now deprecated in favor of `nn.init.xavier_uniform_`.\n",
      "  nn.init.xavier_uniform(self.weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train Loss 1.0966 Acc 0.5167 | Val Loss 1.0963 Acc 0.4640\n",
      "Epoch 002 | Train Loss 1.0896 Acc 0.5167 | Val Loss 1.0931 Acc 0.5600\n",
      "Epoch 003 | Train Loss 1.0860 Acc 0.6000 | Val Loss 1.0874 Acc 0.6860\n",
      "Epoch 004 | Train Loss 1.0757 Acc 0.7000 | Val Loss 1.0798 Acc 0.6340\n",
      "Epoch 005 | Train Loss 1.0676 Acc 0.7000 | Val Loss 1.0727 Acc 0.5640\n",
      "Epoch 006 | Train Loss 1.0550 Acc 0.7167 | Val Loss 1.0674 Acc 0.5740\n",
      "Epoch 007 | Train Loss 1.0441 Acc 0.8000 | Val Loss 1.0629 Acc 0.6300\n",
      "Epoch 008 | Train Loss 1.0307 Acc 0.7500 | Val Loss 1.0584 Acc 0.6580\n",
      "Epoch 009 | Train Loss 1.0344 Acc 0.7833 | Val Loss 1.0537 Acc 0.6820\n",
      "Epoch 010 | Train Loss 1.0180 Acc 0.7833 | Val Loss 1.0487 Acc 0.6980\n",
      "Epoch 011 | Train Loss 1.0080 Acc 0.8000 | Val Loss 1.0427 Acc 0.7040\n",
      "Epoch 012 | Train Loss 0.9925 Acc 0.8167 | Val Loss 1.0354 Acc 0.7020\n",
      "Epoch 013 | Train Loss 0.9831 Acc 0.8833 | Val Loss 1.0272 Acc 0.7080\n",
      "Epoch 014 | Train Loss 0.9557 Acc 0.8833 | Val Loss 1.0183 Acc 0.7020\n",
      "Epoch 015 | Train Loss 0.9628 Acc 0.8667 | Val Loss 1.0094 Acc 0.7080\n",
      "Epoch 016 | Train Loss 0.9318 Acc 0.8500 | Val Loss 1.0002 Acc 0.7100\n",
      "Epoch 017 | Train Loss 0.9193 Acc 0.8500 | Val Loss 0.9914 Acc 0.7120\n",
      "Epoch 018 | Train Loss 0.8966 Acc 0.9000 | Val Loss 0.9830 Acc 0.7120\n",
      "Epoch 019 | Train Loss 0.8986 Acc 0.8833 | Val Loss 0.9746 Acc 0.7080\n",
      "Epoch 020 | Train Loss 0.8875 Acc 0.8333 | Val Loss 0.9671 Acc 0.7100\n",
      "Epoch 021 | Train Loss 0.8610 Acc 0.9000 | Val Loss 0.9596 Acc 0.7120\n",
      "Epoch 022 | Train Loss 0.8519 Acc 0.8833 | Val Loss 0.9520 Acc 0.7200\n",
      "Epoch 023 | Train Loss 0.8377 Acc 0.8833 | Val Loss 0.9444 Acc 0.7240\n",
      "Epoch 024 | Train Loss 0.8220 Acc 0.8500 | Val Loss 0.9364 Acc 0.7220\n",
      "Epoch 025 | Train Loss 0.8091 Acc 0.8833 | Val Loss 0.9285 Acc 0.7200\n",
      "Epoch 026 | Train Loss 0.7888 Acc 0.9000 | Val Loss 0.9204 Acc 0.7180\n",
      "Epoch 027 | Train Loss 0.7962 Acc 0.8667 | Val Loss 0.9127 Acc 0.7240\n",
      "Epoch 028 | Train Loss 0.7694 Acc 0.8833 | Val Loss 0.9051 Acc 0.7220\n",
      "Epoch 029 | Train Loss 0.7666 Acc 0.8500 | Val Loss 0.8966 Acc 0.7240\n",
      "Epoch 030 | Train Loss 0.7434 Acc 0.9333 | Val Loss 0.8869 Acc 0.7300\n",
      "Epoch 031 | Train Loss 0.7216 Acc 0.9167 | Val Loss 0.8777 Acc 0.7280\n",
      "Epoch 032 | Train Loss 0.7243 Acc 0.7833 | Val Loss 0.8689 Acc 0.7420\n",
      "Epoch 033 | Train Loss 0.7051 Acc 0.8833 | Val Loss 0.8609 Acc 0.7440\n",
      "Epoch 034 | Train Loss 0.7048 Acc 0.8500 | Val Loss 0.8540 Acc 0.7420\n",
      "Epoch 035 | Train Loss 0.6684 Acc 0.9500 | Val Loss 0.8478 Acc 0.7400\n",
      "Epoch 036 | Train Loss 0.6606 Acc 0.8833 | Val Loss 0.8419 Acc 0.7340\n",
      "Epoch 037 | Train Loss 0.6421 Acc 0.9500 | Val Loss 0.8365 Acc 0.7440\n",
      "Epoch 038 | Train Loss 0.6370 Acc 0.9333 | Val Loss 0.8314 Acc 0.7360\n",
      "Epoch 039 | Train Loss 0.6151 Acc 0.9333 | Val Loss 0.8259 Acc 0.7500\n",
      "Epoch 040 | Train Loss 0.6340 Acc 0.9000 | Val Loss 0.8202 Acc 0.7500\n",
      "Epoch 041 | Train Loss 0.5834 Acc 0.9333 | Val Loss 0.8135 Acc 0.7500\n",
      "Epoch 042 | Train Loss 0.6037 Acc 0.9167 | Val Loss 0.8066 Acc 0.7540\n",
      "Epoch 043 | Train Loss 0.5955 Acc 0.9000 | Val Loss 0.8000 Acc 0.7540\n",
      "Epoch 044 | Train Loss 0.5481 Acc 0.9500 | Val Loss 0.7936 Acc 0.7560\n",
      "Epoch 045 | Train Loss 0.5330 Acc 0.9333 | Val Loss 0.7873 Acc 0.7520\n",
      "Epoch 046 | Train Loss 0.5129 Acc 0.9833 | Val Loss 0.7811 Acc 0.7520\n",
      "Epoch 047 | Train Loss 0.5577 Acc 0.9167 | Val Loss 0.7752 Acc 0.7560\n",
      "Epoch 048 | Train Loss 0.5254 Acc 0.8667 | Val Loss 0.7694 Acc 0.7600\n",
      "Epoch 049 | Train Loss 0.5521 Acc 0.9167 | Val Loss 0.7643 Acc 0.7600\n",
      "Epoch 050 | Train Loss 0.5057 Acc 0.9167 | Val Loss 0.7598 Acc 0.7600\n",
      "Epoch 051 | Train Loss 0.5176 Acc 0.9333 | Val Loss 0.7561 Acc 0.7620\n",
      "Epoch 052 | Train Loss 0.4694 Acc 0.9667 | Val Loss 0.7522 Acc 0.7680\n",
      "Epoch 053 | Train Loss 0.4655 Acc 0.9500 | Val Loss 0.7495 Acc 0.7700\n",
      "Epoch 054 | Train Loss 0.4695 Acc 0.9667 | Val Loss 0.7476 Acc 0.7660\n",
      "Epoch 055 | Train Loss 0.4454 Acc 0.9667 | Val Loss 0.7444 Acc 0.7680\n",
      "Epoch 056 | Train Loss 0.4967 Acc 0.9000 | Val Loss 0.7387 Acc 0.7700\n",
      "Epoch 057 | Train Loss 0.4331 Acc 0.9500 | Val Loss 0.7323 Acc 0.7760\n",
      "Epoch 058 | Train Loss 0.4482 Acc 0.9500 | Val Loss 0.7260 Acc 0.7720\n",
      "Epoch 059 | Train Loss 0.4209 Acc 0.9500 | Val Loss 0.7201 Acc 0.7820\n",
      "Epoch 060 | Train Loss 0.4550 Acc 0.9500 | Val Loss 0.7157 Acc 0.7820\n",
      "Epoch 061 | Train Loss 0.4352 Acc 0.9667 | Val Loss 0.7117 Acc 0.7820\n",
      "Epoch 062 | Train Loss 0.4264 Acc 0.9667 | Val Loss 0.7079 Acc 0.7800\n",
      "Epoch 063 | Train Loss 0.4332 Acc 0.9167 | Val Loss 0.7046 Acc 0.7840\n",
      "Epoch 064 | Train Loss 0.4226 Acc 0.9667 | Val Loss 0.7018 Acc 0.7860\n",
      "Epoch 065 | Train Loss 0.3905 Acc 0.9500 | Val Loss 0.6993 Acc 0.7840\n",
      "Epoch 066 | Train Loss 0.4140 Acc 0.9500 | Val Loss 0.6961 Acc 0.7820\n",
      "Epoch 067 | Train Loss 0.4059 Acc 0.9833 | Val Loss 0.6934 Acc 0.7840\n",
      "Epoch 068 | Train Loss 0.3904 Acc 0.9500 | Val Loss 0.6908 Acc 0.7880\n",
      "Epoch 069 | Train Loss 0.3535 Acc 1.0000 | Val Loss 0.6879 Acc 0.7880\n",
      "Epoch 070 | Train Loss 0.3538 Acc 0.9500 | Val Loss 0.6853 Acc 0.7980\n",
      "Epoch 071 | Train Loss 0.4297 Acc 0.9167 | Val Loss 0.6827 Acc 0.7980\n",
      "Epoch 072 | Train Loss 0.3722 Acc 0.9667 | Val Loss 0.6808 Acc 0.7960\n",
      "Epoch 073 | Train Loss 0.3674 Acc 0.9500 | Val Loss 0.6791 Acc 0.7940\n",
      "Epoch 074 | Train Loss 0.3613 Acc 0.9500 | Val Loss 0.6768 Acc 0.7960\n",
      "Epoch 075 | Train Loss 0.3555 Acc 0.9333 | Val Loss 0.6745 Acc 0.7960\n",
      "Epoch 076 | Train Loss 0.3651 Acc 0.9500 | Val Loss 0.6732 Acc 0.7900\n",
      "Epoch 077 | Train Loss 0.3226 Acc 0.9833 | Val Loss 0.6722 Acc 0.7880\n",
      "Epoch 078 | Train Loss 0.3144 Acc 0.9500 | Val Loss 0.6717 Acc 0.7920\n",
      "Epoch 079 | Train Loss 0.3193 Acc 0.9667 | Val Loss 0.6710 Acc 0.7920\n",
      "Epoch 080 | Train Loss 0.3310 Acc 0.9833 | Val Loss 0.6695 Acc 0.7900\n",
      "Early stopping at epoch 80. Best val acc: 0.7980\n",
      "Test  | Loss 0.6914 Acc 0.7670\n"
     ]
    }
   ],
   "source": [
    "#Pubmed\n",
    "model, m = train_gcn(dataset='pubmed')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
